import argparse
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ======================== í‰ê°€ í•¨ìˆ˜ ========================

def _validate_input(answer_df, submission_df):
    # â‘  ì»¬ëŸ¼ ê°œìˆ˜Â·ì´ë¦„ ì¼ì¹˜ ì—¬ë¶€
    if len(answer_df.columns) != len(submission_df.columns) or not all(answer_df.columns == submission_df.columns):
        raise ValueError("The columns of the answer and submission dataframes do not match.")

    # â‘¡ í•„ìˆ˜ ì»¬ëŸ¼ì— NaN ì¡´ì¬ ì—¬ë¶€
    if submission_df.isnull().values.any():
        raise ValueError("The submission dataframe contains missing values.")

    # â‘¢ pair ì¤‘ë³µ ì—¬ë¶€
    pairs = list(zip(submission_df["leading_item_id"], submission_df["following_item_id"]))
    if len(pairs) != len(set(pairs)):
        raise ValueError("The submission dataframe contains duplicate (leading_item_id, following_item_id) pairs.")


def comovement_f1(answer_df, submission_df):
    """ê³µí–‰ì„±ìŒ F1 ê³„ì‚°"""
    ans = answer_df[["leading_item_id", "following_item_id"]].copy()
    sub = submission_df[["leading_item_id", "following_item_id"]].copy()

    ans["pair"] = list(zip(ans["leading_item_id"], ans["following_item_id"]))
    sub["pair"] = list(zip(sub["leading_item_id"], sub["following_item_id"]))

    G = set(ans["pair"])
    P = set(sub["pair"])

    tp = len(G & P)
    fp = len(P - G)
    fn = len(G - P)

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

    return f1


def comovement_nmae(answer_df, submission_df, eps=1e-6):
    """
    ì „ì²´ U = G âˆª Pì— ëŒ€í•œ clipped NMAE ê³„ì‚°
    """
    ans = answer_df[["leading_item_id", "following_item_id", "value"]].copy()
    sub = submission_df[["leading_item_id", "following_item_id", "value"]].copy()

    ans["pair"] = list(zip(ans["leading_item_id"], ans["following_item_id"]))
    sub["pair"] = list(zip(sub["leading_item_id"], sub["following_item_id"]))

    G = set(ans["pair"])
    P = set(sub["pair"])
    U = G | P

    ans_val = dict(zip(ans["pair"], ans["value"]))
    sub_val = dict(zip(sub["pair"], sub["value"]))

    errors = []
    for pair in U:
        if pair in G and pair in P:
            # ì •ìˆ˜ ë³€í™˜(ë°˜ì˜¬ë¦¼)
            y_true = int(round(float(ans_val[pair])))
            y_pred = int(round(float(sub_val[pair])))
            rel_err = abs(y_true - y_pred) / (abs(y_true) + eps)
            rel_err = min(rel_err, 1.0)  # ì˜¤ì°¨ 100% ì´ìƒì€ 100%ë¡œ ê°„ì£¼
        else:
            rel_err = 1.0  # FN, FPëŠ” ì˜¤ì°¨ 100%
        errors.append(rel_err)

    return np.mean(errors) if errors else 1.0


def comovement_score(answer_df, submission_df):
    _validate_input(answer_df, submission_df)
    S1 = comovement_f1(answer_df, submission_df)
    nmae_full = comovement_nmae(answer_df, submission_df, 1e-6)
    S2 = 1 - nmae_full
    score = 0.6 * S1 + 0.4 * S2
    return score


# ======================== Dataset ========================

class PairSeqDataset(Dataset):
    """
    (A,B) ìŒì˜ ì‹œê³„ì—´ ìœˆë„ìš°ë¥¼ ëª¨ì•„ë†“ì€ Dataset.
    X: [seq_len, input_dim]  (ì˜ˆ: [input_len, 4])  (A,B ê°’ + ì›” sin/cos)
    y: scalar (log1p(B_next_value))
    """
    def __init__(self, X, y):
        # X: (N, seq_len, input_dim)
        # y: (N,)
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ======================== ìœ í‹¸ í•¨ìˆ˜ ========================

def build_time_index(monthly):
    """
    year, month ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ íƒ€ì„ë¼ì¸ ì¸ë±ìŠ¤ ìƒì„±.
    ë°˜í™˜:
      - time_index: [ (year, month), ... ] ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸
      - ym2idx: { (year,month) -> idx } ë§¤í•‘
    """
    df = monthly[["year", "month"]].drop_duplicates().copy()
    df = df.sort_values(["year", "month"])
    time_index = list(zip(df["year"], df["month"]))
    ym2idx = {ym: i for i, ym in enumerate(time_index)}
    return time_index, ym2idx


def build_pair_series(monthly, time_index, ym2idx):
    """
    ì›”ë³„ ì§‘ê³„ ë°ì´í„° monthly ë¡œë¶€í„°
    item_id ë³„ë¡œ ì „ì²´ íƒ€ì„ë¼ì¸ì— ë§ëŠ” value ì‹œê³„ì—´ ìƒì„±.

    ë°˜í™˜:
      - values_matrix: shape (T, n_items), (year,month ìˆœ) value (log1p ë³€í™˜ ì „ì˜ ì›ê°’)
      - item_ids: columnì— í•´ë‹¹í•˜ëŠ” item_id ë¦¬ìŠ¤íŠ¸
      - type_dict: { item_id -> type }
      - hs4_dict: { item_id -> hs4 }
    """
    T = len(time_index)

    # item_id ëª©ë¡ (ë¬¸ìì—´ë¡œ ê°€ì •)
    item_ids = sorted(monthly["item_id"].astype(str).unique().tolist())
    n_items = len(item_ids)

    # ê°’ ë§¤íŠ¸ë¦­ìŠ¤ ë° ë©”íƒ€ì •ë³´
    values_matrix = np.zeros((T, n_items), dtype=np.float64)

    type_dict = {}
    hs4_dict = {}

    # itemë³„ meta ë¨¼ì € ëª¨ìœ¼ê¸°
    meta = (monthly
            .groupby("item_id", as_index=False)
            .agg({"type": "first", "hs4": "first"}))

    for _, row in meta.iterrows():
        item = str(row["item_id"])
        type_dict[item] = row["type"]
        hs4_dict[item] = row["hs4"]

    # ì‹¤ì œ value ì±„ìš°ê¸°
    # (item_id, year, month) ê¸°ì¤€ìœ¼ë¡œ value í•©ì‚°ì€ ì´ë¯¸ ë¼ ìˆë‹¤ê³  ê°€ì •
    for j, item in enumerate(item_ids):
        sub = monthly[monthly["item_id"].astype(str) == item]
        for _, r in sub.iterrows():
            ym = (int(r["year"]), int(r["month"]))
            idx = ym2idx[ym]
            values_matrix[idx, j] = float(r["value"])

    return values_matrix, item_ids, type_dict, hs4_dict


def build_train_windows_for_pairs(values_matrix, item_ids, time_index,
                                  candidate_pairs, input_len=12):
    """
    ëª¨ë“  candidate (A,B) ìŒì— ëŒ€í•´
    ê¸¸ì´ input_len ì˜ ìœˆë„ìš°ë¡œë¶€í„° B_next ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•™ìŠµ ìƒ˜í”Œ ìƒì„±.

    - feature: [A_value, B_value, month_sin, month_cos]
      (ëª¨ë‘ log1p ë³€í™˜í•´ì„œ ëª¨ë¸ì— ë„£ìŒ)
    - target: log1p(B_next_value)

    ë°˜í™˜:
      - X: shape (N_samples, input_len, 4)
      - y: shape (N_samples,)
      - time_idx_arr: ê° ìƒ˜í”Œì˜ targetì´ ìœ„ì¹˜í•œ time index (ì¼ë¶€ split ìš©)
    """
    T, n_items = values_matrix.shape

    # item_id -> column index ë§µí•‘ (ë¬¸ìì—´ í‚¤)
    item2col = {str(item_id): j for j, item_id in enumerate(item_ids)}

    # month of yearë¥¼ ê¸°ë°˜ìœ¼ë¡œ sin/cos feature ìƒì„±
    months = np.array([m for (_, m) in time_index], dtype=np.float32)  # 1~12
    month_rad = 2 * np.pi * (months - 1) / 12.0
    month_sin = np.sin(month_rad)
    month_cos = np.cos(month_rad)

    X_list = []
    y_list = []
    time_idx_list = []

    for _, row in candidate_pairs.iterrows():
        A = str(row["leading_item_id"])
        B = str(row["following_item_id"])

        if A not in item2col or B not in item2col:
            continue

        col_A = item2col[A]
        col_B = item2col[B]

        series_A = values_matrix[:, col_A]
        series_B = values_matrix[:, col_B]

        # log1p ë³€í™˜
        logA = np.log1p(series_A)
        logB = np.log1p(series_B)

        # k: target ì‹œì  index (B_next)
        # window: [k-input_len .. k-1] ì‚¬ìš©
        # këŠ” ìµœì†Œ input_len, ìµœëŒ€ T-1 ê¹Œì§€ (T-1ì€ ë§ˆì§€ë§‰ ê´€ì¸¡)
        for k in range(input_len, T):
            # ì…ë ¥ ìœˆë„ìš° ë²”ìœ„
            start = k - input_len
            end = k  # k-1 ê¹Œì§€

            # feature ì‹œí€€ìŠ¤ êµ¬ì„±: [A, B, sin, cos]
            window_A = logA[start:end]             # (input_len,)
            window_B = logB[start:end]             # (input_len,)
            window_sin = month_sin[start:end]      # (input_len,)
            window_cos = month_cos[start:end]      # (input_len,)

            # stack -> (input_len, 4)
            window_feat = np.stack(
                [window_A, window_B, window_sin, window_cos],
                axis=-1
            )

            # target: B_next (ì‹œì  k)ì˜ log1p(B)
            target = logB[k]  # scalar

            X_list.append(window_feat)
            y_list.append(target)
            time_idx_list.append(k)

    if not X_list:
        raise RuntimeError("No training windows were generated. Check input_len and data range.")

    X = np.stack(X_list, axis=0)  # (N, input_len, 4)
    y = np.array(y_list, dtype=np.float64)
    time_idx_arr = np.array(time_idx_list, dtype=np.int64)

    return X, y, time_idx_arr


# ======================== Transformer ëª¨ë¸ ========================

class TimeSeriesTransformer(nn.Module):
    """
    ê°„ë‹¨í•œ Transformer Encoder ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸.
    ì…ë ¥: (batch, seq_len, input_dim)
    ì¶œë ¥: scalar (batch,) - ë§ˆì§€ë§‰ í† í° representation ì—ì„œ ì˜ˆì¸¡
    """
    def __init__(self, input_dim=4, d_model=128, nhead=8,
                 num_layers=4, dim_feedforward=256, dropout=0.1):
        super().__init__()

        self.input_proj = nn.Linear(input_dim, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        self.out_proj = nn.Linear(d_model, 1)

    def forward(self, x):
        """
        x: (batch, seq_len, input_dim)
        """
        h = self.input_proj(x)          # (B, L, d_model)
        h_enc = self.encoder(h)         # (B, L, d_model)
        last_token = h_enc[:, -1, :]    # (B, d_model)
        out = self.out_proj(last_token) # (B, 1)
        return out.squeeze(-1)          # (B,)


# ======================== í•™ìŠµ & ì˜ˆì¸¡ ë£¨í‹´ ========================

def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--monthly", type=str, required=True,
                        help="monthly_agg.csv ê²½ë¡œ")
    parser.add_argument("--pairs", type=str, required=True,
                        help="candidate_pairs_topN.csv ê²½ë¡œ")
    parser.add_argument("--sample_submission", type=str, required=True,
                        help="sample_submission.csv ê²½ë¡œ")
    parser.add_argument("--out_submission", type=str, default="submission_ts_transformer.csv",
                        help="ì¶œë ¥ submission íŒŒì¼ ì´ë¦„")
    parser.add_argument("--answer", type=str, default=None,
                        help="(ì„ íƒ) answer.csv ê²½ë¡œ - ìˆìœ¼ë©´ comovement_score ê³„ì‚°")

    parser.add_argument("--input_len", type=int, default=12,
                        help="ìœˆë„ìš° ê¸¸ì´ (ê³¼ê±° ëª‡ ê°œì›”ì„ ë³¼ì§€)")
    parser.add_argument("--batch_size", type=int, default=512)
    parser.add_argument("--epochs", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--valid_last_n_steps", type=int, default=6,
                        help="ë§ˆì§€ë§‰ time index ê¸°ì¤€ ëª‡ ê°œë¥¼ validation ìœ¼ë¡œ ë‘˜ì§€")
    parser.add_argument("--d_model", type=int, default=128)
    parser.add_argument("--nhead", type=int, default=8)
    parser.add_argument("--num_layers", type=int, default=4)
    parser.add_argument("--dim_feedforward", type=int, default=256)
    parser.add_argument("--dropout", type=float, default=0.1)

    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"â–¶ Using device: {device}")

    # 1) ë°ì´í„° ë¡œë“œ
    print("â–¶ monthly_agg.csv ë¡œë“œ ì¤‘...")
    monthly = pd.read_csv(args.monthly)
    print(f"  - monthly shape: {monthly.shape}")

    print("â–¶ candidate_pairs_topN.csv ë¡œë“œ ì¤‘...")
    candidate_pairs = pd.read_csv(args.pairs)
    print(f"  - pairs shape: {candidate_pairs.shape}")

    # ğŸ”§ item_id ê³„ì—´ì€ ì „ë¶€ ë¬¸ìì—´ë¡œ í†µì¼
    monthly["item_id"] = monthly["item_id"].astype(str)
    candidate_pairs["leading_item_id"] = candidate_pairs["leading_item_id"].astype(str)
    candidate_pairs["following_item_id"] = candidate_pairs["following_item_id"].astype(str)

    # 2) íƒ€ì„ë¼ì¸ & ì‹œê³„ì—´ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    print("â–¶ íƒ€ì„ë¼ì¸ ë° item ì‹œê³„ì—´ êµ¬ì„± ì¤‘...")
    time_index, ym2idx = build_time_index(monthly)
    values_matrix, item_ids, type_dict, hs4_dict = build_pair_series(
        monthly, time_index, ym2idx
    )

    print(f"  - time steps: {len(time_index)}, items: {len(item_ids)}")

    # 3) ëª¨ë“  (A,B) í›„ë³´ìŒì— ëŒ€í•´ í•™ìŠµ ìœˆë„ìš° ìƒì„±
    print("â–¶ í•™ìŠµìš© ìœˆë„ìš° ìƒì„± ì¤‘...")
    X, y, time_idx_arr = build_train_windows_for_pairs(
        values_matrix, item_ids, time_index,
        candidate_pairs,
        input_len=args.input_len
    )

    print(f"  - total train windows: {X.shape[0]}  (seq_len={X.shape[1]}, feat_dim={X.shape[2]})")

    # 4) train / valid split (time ê¸°ë°˜)
    max_tidx = time_idx_arr.max()
    valid_threshold = max_tidx - args.valid_last_n_steps + 1

    train_mask = time_idx_arr < valid_threshold
    valid_mask = time_idx_arr >= valid_threshold

    X_train = X[train_mask]
    y_train = y[train_mask]
    X_valid = X[valid_mask]
    y_valid = y[valid_mask]

    print(f"  - train windows: {X_train.shape[0]}, valid windows: {X_valid.shape[0]}")

    train_dataset = PairSeqDataset(X_train, y_train)
    valid_dataset = PairSeqDataset(X_valid, y_valid)

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        drop_last=False
    )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        drop_last=False
    )

    # 5) ëª¨ë¸ ìƒì„±
    model = TimeSeriesTransformer(
        input_dim=X.shape[2],
        d_model=args.d_model,
        nhead=args.nhead,
        num_layers=args.num_layers,
        dim_feedforward=args.dim_feedforward,
        dropout=args.dropout
    ).to(device)

    criterion = nn.L1Loss()  # MAE ê¸°ë°˜
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )

    best_valid_loss = float("inf")
    patience = 20
    no_improve = 0

    # 6) í•™ìŠµ ë£¨í”„
    print("â–¶ í•™ìŠµ ì‹œì‘...")
    for epoch in range(1, args.epochs + 1):
        # ---- train ----
        model.train()
        train_loss_sum = 0.0
        n_train = 0

        for xb, yb in train_loader:
            xb = xb.to(device)
            yb = yb.to(device)

            optimizer.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            bs = xb.size(0)
            train_loss_sum += loss.item() * bs
            n_train += bs

        train_loss = train_loss_sum / max(n_train, 1)

        # ---- valid ----
        model.eval()
        valid_loss_sum = 0.0
        n_valid = 0
        with torch.no_grad():
            for xb, yb in valid_loader:
                xb = xb.to(device)
                yb = yb.to(device)
                pred = model(xb)
                loss = criterion(pred, yb)
                bs = xb.size(0)
                valid_loss_sum += loss.item() * bs
                n_valid += bs

        valid_loss = valid_loss_sum / max(n_valid, 1)

        print(f"[Epoch {epoch:03d}] train_loss={train_loss:.5f}  valid_loss={valid_loss:.5f}")

        # early stopping
        if valid_loss < best_valid_loss - 1e-4:
            best_valid_loss = valid_loss
            no_improve = 0
            torch.save(model.state_dict(), "best_ts_transformer.pth")
        else:
            no_improve += 1
            if no_improve >= patience:
                print("â–¶ Early stopping triggered.")
                break

    print(f"â–¶ Best valid loss: {best_valid_loss:.5f}")
    # best ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(torch.load("best_ts_transformer.pth", map_location=device))
    model.eval()

    # 7) 2025-08 ì˜ˆì¸¡ (ê° (A,B) í›„ë³´ìŒ)
    print("â–¶ 2025-08 ì˜ˆì¸¡ìš© ìœˆë„ìš° ìƒì„± ì¤‘...")

    T = len(time_index)
    months = np.array([m for (_, m) in time_index], dtype=np.float32)
    month_rad = 2 * np.pi * (months - 1) / 12.0
    month_sin = np.sin(month_rad)
    month_cos = np.cos(month_rad)

    # item -> column index (ë¬¸ìì—´ í‚¤)
    item2col = {str(item_id): j for j, item_id in enumerate(item_ids)}

    pred_rows = []

    for _, row in candidate_pairs.iterrows():
        A = str(row["leading_item_id"])
        B = str(row["following_item_id"])

        if A not in item2col or B not in item2col:
            continue

        col_A = item2col[A]
        col_B = item2col[B]

        series_A = values_matrix[:, col_A]
        series_B = values_matrix[:, col_B]

        logA = np.log1p(series_A)
        logB = np.log1p(series_B)

        # ë§ˆì§€ë§‰ input_len ê°œì›” (ë index = T-1) ì‚¬ìš©
        if T < args.input_len:
            continue

        start = T - args.input_len
        end = T  # T-1 ê¹Œì§€

        window_A = logA[start:end]
        window_B = logB[start:end]
        window_sin = month_sin[start:end]
        window_cos = month_cos[start:end]

        window_feat = np.stack(
            [window_A, window_B, window_sin, window_cos],
            axis=-1
        )  # (input_len, 4)

        x = torch.from_numpy(window_feat.astype(np.float32)).unsqueeze(0).to(device)  # (1, L, 4)

        with torch.no_grad():
            log_pred_next = model(x).item()  # log1p(pred_value)

        pred_value = np.expm1(log_pred_next)
        if pred_value < 0:
            pred_value = 0.0

        pred_rows.append({
            "leading_item_id": A,
            "following_item_id": B,
            "value": int(round(pred_value))
        })

    pred_df = pd.DataFrame(pred_rows)
    print(f"  - ì˜ˆì¸¡ëœ pair ìˆ˜: {pred_df.shape[0]}")

    # ===== fallback ê°’ ê³„ì‚° (ìµœê·¼ 3ê°œì›” í‰ê·  ê¸°ë°˜) =====
    k = 3
    if values_matrix.shape[0] >= k:
        # ì „ì²´ í‰ê·  (global fallback)
        global_fallback_value = float(values_matrix[-k:, :].mean())
        # itemë³„ ìµœê·¼ kê°œì›” í‰ê· 
        itemwise_recent_mean = values_matrix[-k:, :].mean(axis=0)
    else:
        global_fallback_value = float(values_matrix.mean())
        itemwise_recent_mean = values_matrix.mean(axis=0)

    fallback_dict = {
        str(item_id): float(v)
        for item_id, v in zip(item_ids, itemwise_recent_mean)
    }

    print(f"â–¶ global_fallback_value (ìµœê·¼ {k}ê°œì›” ì „ì²´ í‰ê· ): {global_fallback_value:.2f}")

    # 8) sample_submission ê¸°ë°˜ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±
    print("â–¶ sample_submission ê¸°ë°˜ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
    sub = pd.read_csv(args.sample_submission)

    # ğŸ”§ ë¬¸ìì—´ë¡œ í†µì¼
    sub["leading_item_id"] = sub["leading_item_id"].astype(str)
    sub["following_item_id"] = sub["following_item_id"].astype(str)

    # ì˜ˆì¸¡ dict (ë¬¸ìì—´ í‚¤)
    pair2val = {
        (str(r["leading_item_id"]), str(r["following_item_id"])): int(r["value"])
        for _, r in pred_df.iterrows()
    }

    vals = []
    for a, b in zip(sub["leading_item_id"].values, sub["following_item_id"].values):
        key = (str(a), str(b))
        if key in pair2val:
            # candidate_pairsì— ìˆì—ˆê³  Transformerê°€ ì˜ˆì¸¡í•œ ìŒ
            vals.append(pair2val[key])
        else:
            # candidate_pairsì—ëŠ” ì—†ì§€ë§Œ ì •ë‹µì— ìˆì„ ìˆ˜ë„ ìˆëŠ” ìŒ
            fid = str(b)
            if fid in fallback_dict:
                vals.append(int(round(fallback_dict[fid])))
            else:
                vals.append(int(round(global_fallback_value)))

    sub["value"] = vals

    sub.to_csv(args.out_submission, index=False)
    print(f"â–¶ ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {args.out_submission}")
    print(f"  - shape: {sub.shape}")

    # 9) answer.csv ê°€ ìˆë‹¤ë©´ ìŠ¤ì½”ì–´ ê³„ì‚°
    if args.answer is not None:
        print("â–¶ answer.csv ê¸°ë°˜ comovement_score ê³„ì‚° ì¤‘...")
        answer_df = pd.read_csv(args.answer)
        try:
            score = comovement_score(answer_df, sub)
            f1 = comovement_f1(answer_df, sub)
            nmae = comovement_nmae(answer_df, sub)
            print(f"  - F1   : {f1:.6f}")
            print(f"  - NMAE : {nmae:.6f}")
            print(f"  - Score: {score:.6f}")
        except Exception as e:
            print(f"  - ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
